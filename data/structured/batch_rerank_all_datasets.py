#!/usr/bin/env python3
"""
æ‰¹é‡é‡æ’åºè„šæœ¬ - å¤„ç†structuredæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ•°æ®é›†

åŠŸèƒ½:
1. è‡ªåŠ¨å‘ç°å¹¶å¤„ç†æ‰€æœ‰æ•°æ®é›†
2. å¯¹æ¯ä¸ªæ•°æ®é›†æ‰§è¡ŒSimHashé‡æ’åº
3. ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š
4. è¾“å‡ºè®ºæ–‡çº§åˆ«çš„ç»“æœæ±‡æ€»

è¾“å‡ºæ–‡ä»¶:
- æ¯ä¸ªæ•°æ®é›†: {dataset}_reranked_results_k50.parquet
- æ¯ä¸ªæ•°æ®é›†: {dataset}_evaluation_report.json  
- æ±‡æ€»æŠ¥å‘Š: all_datasets_rerank_summary.json
- æ±‡æ€»æŠ¥å‘Š: all_datasets_rerank_summary.csv

ä½¿ç”¨æ–¹æ³•:
cd data/structured/
python batch_rerank_all_datasets.py [--alpha 0.6] [--beta 0.4] [--sample_only]
"""

import os
import pandas as pd
import numpy as np
import json
import time
import argparse
from typing import Dict, List, Tuple, Optional
from pathlib import Path
import sys

# å¯¼å…¥é‡æ’åºç›¸å…³æ¨¡å—
sys.path.append('./abt_buy')  # æ·»åŠ abt_buyè·¯å¾„
from simhash_reranker import SimHashReranker, RerankerEvaluator, RerankerConfig


class UniversalDatasetEvaluator:
    """é€šç”¨æ•°æ®é›†è¯„ä¼°å™¨ï¼ˆåŸºäºtest_rerank.pyï¼‰"""
    
    def __init__(self, config: RerankerConfig, dataset_path: Path):
        self.config = config
        self.dataset_name = dataset_path.name
        self.dataset_path = dataset_path
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        print(f"ğŸ“ åŠ è½½ {self.dataset_name} æ•°æ®é›†...")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        required_files = [
            'table_a.parquet', 'table_b.parquet', 
            'sparkly_results_k50.parquet', 'optimization_result.json'
        ]
        
        missing_files = [f for f in required_files if not (self.dataset_path / f).exists()]
        if missing_files:
            raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")

        # åŠ è½½æ•°æ®
        self.table_a = pd.read_parquet(self.dataset_path / 'table_a.parquet')
        self.table_b = pd.read_parquet(self.dataset_path / 'table_b.parquet')
        self.search_results = pd.read_parquet(self.dataset_path / 'sparkly_results_k50.parquet')
        
        # åŠ è½½çœŸå®æ ‡ç­¾
        try:
            gold_df = pd.read_parquet(self.dataset_path / 'gold.parquet')
            self.gold_mapping = dict(zip(gold_df['id2'], gold_df['id1']))
            print(f"âœ… åŠ è½½äº† {len(self.gold_mapping)} ä¸ªçœŸå®æ ‡ç­¾")
        except FileNotFoundError:
            print("âš ï¸ æœªæ‰¾åˆ°gold.parquetæ–‡ä»¶ï¼Œå°†æ— æ³•è¿›è¡Œå‡†ç¡®æ€§è¯„ä¼°")
            self.gold_mapping = {}
        
        print(f"âœ… Table A: {len(self.table_a)} æ¡è®°å½•")
        print(f"âœ… Table B: {len(self.table_b)} æ¡è®°å½•") 
        print(f"âœ… æœç´¢ç»“æœ: {len(self.search_results)} ä¸ªæŸ¥è¯¢")
        
        # åˆ›å»ºå¿«é€Ÿæ˜ å°„
        self.table_a_dict = self.table_a.set_index('_id').to_dict('index')
        self.table_b_dict = self.table_b.set_index('_id').to_dict('index')
        
    def initialize_reranker(self):
        """åˆå§‹åŒ–é‡æ’åºå™¨"""
        print("âš™ï¸ åˆå§‹åŒ–SimHashé‡æ’åºå™¨...")
        
        self.reranker = SimHashReranker(self.config)
        self.reranker.load_optimization_result(str(self.dataset_path / 'optimization_result.json'))
        self.evaluator = RerankerEvaluator()
        
        print(f"ğŸ”§ é…ç½®å‚æ•°: Î±={self.config.alpha} (BM25), Î²={self.config.beta} (SimHash)")
    
    def process_all_queries(self):
        """å¤„ç†æ‰€æœ‰æŸ¥è¯¢"""
        print(f"\nğŸš€ å¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰ {len(self.search_results)} ä¸ªæŸ¥è¯¢...")
        
        reranked_results = []
        evaluation_metrics = []
        total_rerank_time = 0
        success_count = 0
        error_count = 0
        
        for idx, row in self.search_results.iterrows():
            # æ˜¾ç¤ºè¿›åº¦
            if idx % max(1, len(self.search_results) // 20) == 0:
                progress = idx / len(self.search_results) * 100
                print(f"  è¿›åº¦: {idx}/{len(self.search_results)} ({progress:.1f}%)")
            
            # å¤„ç†æŸ¥è¯¢
            result = self._process_single_query(row)
            
            if result['success']:
                success_count += 1
                total_rerank_time += result['processing_time']
                
                # æ„å»ºé‡æ’åºç»“æœï¼ˆç¡®ä¿ç±»å‹æ­£ç¡®ï¼‰
                reranked_row = {
                    '_id': int(result['query_id']),
                    'ids': [int(x) for x in result['reranked_ids']],
                    'scores': [float(x) for x in result['reranked_scores']],
                    'search_time': float(row.get('search_time', 0)),
                    'rerank_time': float(result['processing_time'])
                }
                reranked_results.append(reranked_row)
                
                # è¯„ä¼°æŒ‡æ ‡
                if result['metrics'] and result['metrics']['has_ground_truth']:
                    evaluation_metrics.append(result['metrics'])
            else:
                error_count += 1
                if error_count <= 5:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                    print(f"  âš ï¸ æŸ¥è¯¢ {row['_id']} å¤„ç†å¤±è´¥: {result.get('error', 'Unknown error')}")
        
        print(f"âœ… å¤„ç†å®Œæˆ: {success_count}/{len(self.search_results)} æŸ¥è¯¢æˆåŠŸ")
        if error_count > 0:
            print(f"âš ï¸ {error_count} ä¸ªæŸ¥è¯¢å¤„ç†å¤±è´¥")
        
        return {
            'reranked_results': reranked_results,
            'evaluation_metrics': evaluation_metrics,
            'success_count': success_count,
            'total_rerank_time': total_rerank_time,
            'error_count': error_count
        }
    
    def _process_single_query(self, row: pd.Series) -> Dict:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        query_id = row['_id']
        candidate_ids = row['ids']
        original_scores = row['scores']
        
        # è·å–æŸ¥è¯¢è®°å½•
        if query_id not in self.table_b_dict:
            return {'success': False, 'error': f'Query ID {query_id} not found'}
            
        query_record = self.table_b_dict[query_id]
        
        # è·å–å€™é€‰è®°å½•
        candidate_records = []
        valid_candidates = []
        valid_scores = []
        
        for i, cand_id in enumerate(candidate_ids):
            if cand_id in self.table_a_dict:
                candidate_records.append(self.table_a_dict[cand_id])
                valid_candidates.append(cand_id)
                valid_scores.append(original_scores[i])
        
        if not candidate_records:
            return {'success': False, 'error': 'No valid candidates'}
        
        # æ‰§è¡Œé‡æ’åº
        try:
            ranking_indices, fused_scores, debug_info = self.reranker.rerank_candidates(
                query_record, candidate_records, valid_scores
            )
            
            reranked_ids = [valid_candidates[i] for i in ranking_indices]
            
            # è¯„ä¼°æŒ‡æ ‡
            metrics = None
            if query_id in self.gold_mapping:
                metrics = self.evaluator.calculate_metrics(
                    query_id, valid_candidates, reranked_ids, self.gold_mapping
                )
            
            return {
                'success': True,
                'query_id': query_id,
                'reranked_ids': reranked_ids,
                'reranked_scores': fused_scores,
                'processing_time': debug_info['processing_time'],
                'metrics': metrics
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def save_results(self, results: Dict) -> Dict:
        """ä¿å­˜ç»“æœå¹¶ç”ŸæˆæŠ¥å‘Š"""
        # ä¿å­˜é‡æ’åºç»“æœ
        if results['reranked_results']:
            reranked_df = pd.DataFrame(results['reranked_results'])
            output_file = self.dataset_path / f"{self.dataset_name}_reranked_results_k50.parquet"
            reranked_df.to_parquet(output_file)
            print(f"ğŸ’¾ é‡æ’åºç»“æœå·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = self._generate_evaluation_report(
            results['evaluation_metrics'], 
            results['success_count'], 
            results['total_rerank_time']
        )
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Šï¼ˆç¡®ä¿JSONåºåˆ—åŒ–å®‰å…¨ï¼‰
        report_file = self.dataset_path / f"{self.dataset_name}_evaluation_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return evaluation_report
    
    def _generate_evaluation_report(self, metrics_list: List[Dict], 
                                  success_count: int, total_rerank_time: float) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            'dataset': self.dataset_name,
            'total_queries': len(self.search_results),
            'successful_queries': success_count,
            'queries_with_ground_truth': len(metrics_list),
            'total_rerank_time': float(total_rerank_time),
            'avg_rerank_time': float(total_rerank_time / success_count if success_count > 0 else 0),
            'config': self.config.__dict__,
            'evaluation_available': len(metrics_list) > 0
        }
        
        if metrics_list:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡ï¼ˆç¡®ä¿ç±»å‹å®‰å…¨ï¼‰
            for k in [1, 5, 10, 20]:
                orig_recalls = [m[f'original_recall@{k}'] for m in metrics_list]
                rerank_recalls = [m[f'reranked_recall@{k}'] for m in metrics_list]
                rank_improvements = [m[f'rank_improvement@{k}'] for m in metrics_list]
                
                report[f'avg_original_recall@{k}'] = float(np.mean(orig_recalls))
                report[f'avg_reranked_recall@{k}'] = float(np.mean(rerank_recalls))
                report[f'recall_improvement@{k}'] = float(np.mean(rerank_recalls) - np.mean(orig_recalls))
                report[f'avg_rank_improvement@{k}'] = float(np.mean(rank_improvements))
                report[f'positive_improvements@{k}'] = int(sum(1 for x in rank_improvements if x > 0))
                report[f'negative_improvements@{k}'] = int(sum(1 for x in rank_improvements if x < 0))
        
        return report


class MultiDatasetReranker:
    """å¤šæ•°æ®é›†æ‰¹é‡é‡æ’åºå™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    
    def __init__(self, config: RerankerConfig = None):
        self.config = config or RerankerConfig(
            simhash_bits=64,
            alpha=0.6,      # ä½¿ç”¨è®ºæ–‡ä¸­çš„æœ€ä½³å‚æ•°
            beta=0.4,
            use_3gram=True,
            normalize_scores=True
        )
        self.datasets = []
        self.results = {}
        
    def discover_datasets(self, base_dir: str = '.') -> List[Path]:
        """è‡ªåŠ¨å‘ç°æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸ” å‘ç°å¯ç”¨æ•°æ®é›†...")
        
        datasets = []
        base_path = Path(base_dir)
        
        for item in base_path.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                required_files = [
                    'sparkly_results_k50.parquet',
                    'table_a.parquet', 
                    'table_b.parquet',
                    'optimization_result.json'
                ]
                
                if all((item / f).exists() for f in required_files):
                    datasets.append(item)
                    print(f"  âœ… {item.name}")
                else:
                    missing = [f for f in required_files if not (item / f).exists()]
                    print(f"  âŒ {item.name} (ç¼ºå°‘: {', '.join(missing)})")
        
        return sorted(datasets, key=lambda x: x.name)
    
    def process_single_dataset(self, dataset_path: Path) -> Dict:
        """å¤„ç†å•ä¸ªæ•°æ®é›†"""
        dataset_name = dataset_path.name
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¤„ç†æ•°æ®é›†: {dataset_name}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºæ•°æ®é›†è¯„ä¼°å™¨
            evaluator = UniversalDatasetEvaluator(self.config, dataset_path)
            
            # åŠ è½½æ•°æ®
            evaluator.load_data()
            
            # åˆå§‹åŒ–é‡æ’åºå™¨
            evaluator.initialize_reranker()
            
            # å¤„ç†æ‰€æœ‰æŸ¥è¯¢
            results = evaluator.process_all_queries()
            
            # ä¿å­˜ç»“æœ
            evaluation_report = evaluator.save_results(results)
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            if evaluation_report['evaluation_available']:
                print(f"\nğŸ“ˆ {dataset_name} æ€§èƒ½æ”¹å–„:")
                for k in [1, 5, 10]:
                    improvement = evaluation_report[f'recall_improvement@{k}']
                    positive = evaluation_report[f'positive_improvements@{k}']
                    total_eval = evaluation_report['queries_with_ground_truth']
                    print(f"   Recall@{k}: {improvement:+.4f} ({positive}/{total_eval} æŸ¥è¯¢æ”¹å–„)")
            else:
                print(f"âš ï¸ {dataset_name} æ— è¯„ä¼°æ•°æ®ï¼ˆç¼ºå°‘çœŸå®æ ‡ç­¾ï¼‰")
            
            # è¿”å›æ±‡æ€»ä¿¡æ¯
            processing_time = time.time() - start_time
            
            return {
                'dataset': dataset_name,
                'success': True,
                'total_queries': len(evaluator.search_results),
                'successful_queries': results['success_count'],
                'queries_with_ground_truth': len(results['evaluation_metrics']),
                'processing_time': processing_time,
                'avg_rerank_time': results['total_rerank_time'] / results['success_count'] if results['success_count'] > 0 else 0,
                'evaluation_report': evaluation_report,
                'error_count': results['error_count']
            }
            
        except Exception as e:
            print(f"âŒ å¤„ç† {dataset_name} æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'dataset': dataset_name,
                'success': False,
                'error': str(e),
                'processing_time': time.time() - start_time
            }
    
    def process_all_datasets(self) -> Dict:
        """å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
        print("ğŸŒŸ å¼€å§‹æ‰¹é‡å¤„ç†æ‰€æœ‰æ•°æ®é›†")
        print("=" * 80)
        
        # å‘ç°æ•°æ®é›†
        datasets = self.discover_datasets()
        
        if not datasets:
            print("âŒ æœªå‘ç°ä»»ä½•å¯å¤„ç†çš„æ•°æ®é›†!")
            return {}
        
        print(f"\nğŸ“Š å‘ç° {len(datasets)} ä¸ªæ•°æ®é›†: {', '.join([d.name for d in datasets])}")
        print(f"ğŸ”§ ä½¿ç”¨å‚æ•°: Î±={self.config.alpha}, Î²={self.config.beta}, bits={self.config.simhash_bits}")
        
        # å¤„ç†æ¯ä¸ªæ•°æ®é›†
        all_results = {}
        total_start_time = time.time()
        
        for i, dataset_path in enumerate(datasets):
            print(f"\nğŸ¯ å¤„ç†è¿›åº¦: {i+1}/{len(datasets)}")
            result = self.process_single_dataset(dataset_path)
            all_results[dataset_path.name] = result
        
        total_time = time.time() - total_start_time
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = self._generate_summary_report(all_results, total_time)
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        with open('all_datasets_rerank_summary.json', 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ç”ŸæˆCSVæ±‡æ€»è¡¨
        self._generate_csv_summary(all_results)
        
        print(f"\nğŸŠ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆ! æ€»ç”¨æ—¶: {total_time:.2f} ç§’")
        print("ğŸ“„ è¾“å‡ºæ–‡ä»¶:")
        print("   - all_datasets_rerank_summary.json (è¯¦ç»†æ±‡æ€»)")
        print("   - all_datasets_rerank_summary.csv (è¡¨æ ¼æ±‡æ€»)")
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_statistics(summary)
        
        return summary
    
    def _generate_summary_report(self, all_results: Dict, total_time: float) -> Dict:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        
        summary = {
            'experiment_config': self.config.__dict__,
            'total_processing_time': float(total_time),
            'datasets_processed': len(all_results),
            'successful_datasets': sum(1 for r in all_results.values() if r['success']),
            'datasets': {}
        }
        
        # æ”¶é›†æ‰€æœ‰æˆåŠŸçš„æ•°æ®é›†ç»“æœ
        successful_datasets = []
        
        for dataset, result in all_results.items():
            summary['datasets'][dataset] = result
            
            if result['success'] and 'evaluation_report' in result:
                eval_report = result['evaluation_report']
                if eval_report['evaluation_available']:
                    successful_datasets.append(eval_report)
        
        # è®¡ç®—æ•´ä½“ç»Ÿè®¡
        if successful_datasets:
            summary['overall_statistics'] = {}
            
            total_queries = sum(d['queries_with_ground_truth'] for d in successful_datasets)
            summary['overall_statistics']['total_evaluated_queries'] = total_queries
            summary['overall_statistics']['datasets_with_evaluation'] = len(successful_datasets)
            
            # åŠ æƒå¹³å‡å„é¡¹æŒ‡æ ‡
            for k in [1, 5, 10]:
                weighted_orig_recall = 0
                weighted_rerank_recall = 0
                total_positive_improvements = 0
                
                for d in successful_datasets:
                    if d['queries_with_ground_truth'] > 0:
                        weight = d['queries_with_ground_truth'] / total_queries
                        weighted_orig_recall += d[f'avg_original_recall@{k}'] * weight
                        weighted_rerank_recall += d[f'avg_reranked_recall@{k}'] * weight
                        total_positive_improvements += d[f'positive_improvements@{k}']
                
                summary['overall_statistics'][f'weighted_avg_original_recall@{k}'] = float(weighted_orig_recall)
                summary['overall_statistics'][f'weighted_avg_reranked_recall@{k}'] = float(weighted_rerank_recall)
                summary['overall_statistics'][f'overall_recall_improvement@{k}'] = float(weighted_rerank_recall - weighted_orig_recall)
                summary['overall_statistics'][f'total_positive_improvements@{k}'] = int(total_positive_improvements)
        
        return summary
    
    def _generate_csv_summary(self, all_results: Dict):
        """ç”ŸæˆCSVæ ¼å¼çš„æ±‡æ€»è¡¨"""
        
        csv_data = []
        
        for dataset, result in all_results.items():
            if result['success'] and 'evaluation_report' in result:
                eval_report = result['evaluation_report']
                
                row = {
                    'Dataset': dataset,
                    'Success': result['success'],
                    'Total_Queries': eval_report['total_queries'],
                    'Successful_Queries': eval_report['successful_queries'],
                    'Evaluated_Queries': eval_report['queries_with_ground_truth'],
                    'Avg_Rerank_Time_ms': eval_report['avg_rerank_time'] * 1000,
                    'Processing_Time_s': result['processing_time'],
                    'Error_Count': result.get('error_count', 0)
                }
                
                # æ·»åŠ RecallæŒ‡æ ‡
                for k in [1, 5, 10]:
                    if eval_report['evaluation_available']:
                        row[f'Original_Recall@{k}'] = eval_report.get(f'avg_original_recall@{k}', 0)
                        row[f'Reranked_Recall@{k}'] = eval_report.get(f'avg_reranked_recall@{k}', 0)
                        row[f'Recall_Improvement@{k}'] = eval_report.get(f'recall_improvement@{k}', 0)
                        row[f'Positive_Improvements@{k}'] = eval_report.get(f'positive_improvements@{k}', 0)
                    else:
                        row[f'Original_Recall@{k}'] = 'N/A'
                        row[f'Reranked_Recall@{k}'] = 'N/A'
                        row[f'Recall_Improvement@{k}'] = 'N/A'
                        row[f'Positive_Improvements@{k}'] = 'N/A'
                
                csv_data.append(row)
            else:
                # å¤±è´¥çš„æ•°æ®é›†
                row = {
                    'Dataset': dataset,
                    'Success': False,
                    'Error': result.get('error', 'Unknown error'),
                    'Processing_Time_s': result.get('processing_time', 0)
                }
                csv_data.append(row)
        
        if csv_data:
            df = pd.DataFrame(csv_data)
            df.to_csv('all_datasets_rerank_summary.csv', index=False)
            print("ğŸ“Š CSVæ±‡æ€»è¡¨å·²ä¿å­˜: all_datasets_rerank_summary.csv")
    
    def _print_final_statistics(self, summary: Dict):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        if 'overall_statistics' in summary:
            stats = summary['overall_statistics']
            print(f"\nğŸ† æ•´ä½“æ€§èƒ½ç»Ÿè®¡:")
            print(f"   æˆåŠŸå¤„ç†æ•°æ®é›†: {summary['successful_datasets']}/{summary['datasets_processed']}")
            print(f"   æœ‰è¯„ä¼°æ•°æ®çš„æ•°æ®é›†: {stats['datasets_with_evaluation']}")
            print(f"   æ€»è¯„ä¼°æŸ¥è¯¢æ•°: {stats['total_evaluated_queries']}")
            
            for k in [1, 5, 10]:
                improvement = stats[f'overall_recall_improvement@{k}']
                positive = stats[f'total_positive_improvements@{k}']
                total = stats['total_evaluated_queries']
                status = "ğŸŸ¢" if improvement > 0 else "ğŸ”´" if improvement < 0 else "ğŸŸ¡"
                print(f"   {status} Overall Recall@{k}: {improvement:+.4f} ({positive}/{total} æŸ¥è¯¢æ”¹å–„)")


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡é‡æ’åºæ‰€æœ‰structuredæ•°æ®é›†')
    parser.add_argument('--alpha', type=float, default=0.6,
                       help='BM25æƒé‡ (é»˜è®¤: 0.6)')
    parser.add_argument('--beta', type=float, default=0.4,
                       help='SimHashæƒé‡ (é»˜è®¤: 0.4)')
    parser.add_argument('--simhash_bits', type=int, default=64,
                       help='SimHashä½æ•° (é»˜è®¤: 64)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºé…ç½®
    config = RerankerConfig(
        simhash_bits=args.simhash_bits,
        alpha=args.alpha,
        beta=args.beta,
        use_3gram=True,
        normalize_scores=True
    )
    
    # åˆ›å»ºæ‰¹é‡é‡æ’åºå™¨
    reranker = MultiDatasetReranker(config)
    
    # å¤„ç†æ‰€æœ‰æ•°æ®é›†
    summary = reranker.process_all_datasets()


if __name__ == "__main__":
    main() 