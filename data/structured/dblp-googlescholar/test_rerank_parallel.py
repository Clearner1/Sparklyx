#!/usr/bin/env python3
"""
é€šç”¨æ•°æ®é›†é‡æ’åºè¯¦ç»†è¯„ä¼°è„šæœ¬ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰

åŠŸèƒ½:
1. æ‰¹é‡å¤„ç†ä»»æ„æ•°æ®é›†çš„æ‰€æœ‰æŸ¥è¯¢
2. ç”Ÿæˆè¯¦ç»†çš„é‡æ’åºæ•ˆæœè¯„ä¼°æŠ¥å‘Š
3. å¯¹æ¯”åŸå§‹BM25 vs SimHashé‡æ’åºçš„æ€§èƒ½
4. è¾“å‡ºå¯è§†åŒ–çš„ç»“æœåˆ†æ
5. å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæé«˜å¤„ç†é€Ÿåº¦

è¾“å‡ºæ–‡ä»¶:
- {dataset}_reranked_results_k50.parquet: é‡æ’åºç»“æœ
- {dataset}_evaluation_report.json: è¯¦ç»†è¯„ä¼°æŠ¥å‘Š
- {dataset}_sample_cases.json: å…¸å‹æ¡ˆä¾‹åˆ†æ

ä½¿ç”¨æ–¹æ³•:
cd data/structured/{any_dataset}/
python test_rerank_parallel.py [--alpha 0.6] [--beta 0.4] [--sample_size 100] [--processes 4]
"""

import json
import pandas as pd
import numpy as np
import time
import argparse
from typing import Dict, List
from pathlib import Path
from simhash_reranker import SimHashReranker, RerankerEvaluator, RerankerConfig
import multiprocessing as mp


def process_single_query_wrapper(args):
    """å¤„ç†å•ä¸ªæŸ¥è¯¢çš„åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¤šè¿›ç¨‹å¤„ç†"""
    row, table_a_dict, table_b_dict, gold_mapping, config, optimization_result_path = args
    
    try:
        # åˆå§‹åŒ–é‡æ’åºå™¨
        reranker = SimHashReranker(config)
        reranker.load_optimization_result(optimization_result_path)
        evaluator = RerankerEvaluator()
        
        query_id = row['_id']
        candidate_ids = row['ids']
        original_scores = row['scores']
        
        # è·å–æŸ¥è¯¢è®°å½•
        if query_id not in table_b_dict:
            return {'success': False, 'error': f'Query ID {query_id} not found'}
            
        query_record = table_b_dict[query_id]
        
        # è·å–å€™é€‰è®°å½•
        candidate_records = []
        valid_candidates = []
        valid_scores = []
        
        for i, cand_id in enumerate(candidate_ids):
            if cand_id in table_a_dict:
                candidate_records.append(table_a_dict[cand_id])
                valid_candidates.append(cand_id)
                valid_scores.append(original_scores[i])
        
        if not candidate_records:
            return {'success': False, 'error': 'No valid candidates'}
        
        # æ‰§è¡Œé‡æ’åº
        ranking_indices, fused_scores, debug_info = reranker.rerank_candidates(
            query_record, candidate_records, valid_scores
        )
        
        reranked_ids = [valid_candidates[i] for i in ranking_indices]
        
        # è¯„ä¼°æŒ‡æ ‡
        metrics = None
        if query_id in gold_mapping:
            metrics = evaluator.calculate_metrics(
                query_id, valid_candidates, reranked_ids, gold_mapping
            )
        
        return {
            'success': True,
            'query_id': query_id,
            'reranked_ids': reranked_ids,
            'reranked_scores': fused_scores,
            'processing_time': debug_info['processing_time'],
            'metrics': metrics
        }
        
    except Exception as e:
        return {'success': False, 'error': str(e)}


class UniversalDatasetEvaluatorParallel:
    """é€šç”¨æ•°æ®é›†è¯„ä¼°å™¨ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰"""
    
    def __init__(self, config: RerankerConfig = None, num_processes: int = 4, verbose: bool = True):
        self.config = config or RerankerConfig(
            simhash_bits=64,
            alpha=0.6,
            beta=0.4,
            use_3gram=True,
            normalize_scores=True
        )
        self.num_processes = num_processes
        self.verbose = verbose
        # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†åç§°
        self.dataset_name = Path.cwd().name
        
    def load_data(self):
        """åŠ è½½æ•°æ®é›†"""
        if self.verbose:
            print(f"ğŸ“ åŠ è½½ {self.dataset_name} æ•°æ®é›†...")
        
        # æ£€æŸ¥å¿…è¦æ–‡ä»¶
        required_files = [
            'table_a.parquet', 'table_b.parquet', 
            'sparkly_results_k50.parquet', 'optimization_result.json'
        ]
        
        missing_files = [f for f in required_files if not Path(f).exists()]
        if missing_files:
            raise FileNotFoundError(f"ç¼ºå°‘å¿…è¦æ–‡ä»¶: {missing_files}")

        # åŠ è½½æ•°æ®
        self.table_a = pd.read_parquet('table_a.parquet')
        self.table_b = pd.read_parquet('table_b.parquet')
        self.search_results = pd.read_parquet('sparkly_results_k50.parquet')
        
        # åŠ è½½çœŸå®æ ‡ç­¾
        try:
            gold_df = pd.read_parquet('gold.parquet')
            self.gold_mapping = dict(zip(gold_df['id2'], gold_df['id1']))
            if self.verbose:
                print(f"âœ… åŠ è½½äº† {len(self.gold_mapping)} ä¸ªçœŸå®æ ‡ç­¾")
        except FileNotFoundError:
            if self.verbose:
                print("âš ï¸ æœªæ‰¾åˆ°gold.parquetæ–‡ä»¶ï¼Œå°†æ— æ³•è¿›è¡Œå‡†ç¡®æ€§è¯„ä¼°")
            self.gold_mapping = {}
        
        if self.verbose:
            print(f"âœ… Table A: {len(self.table_a)} æ¡è®°å½•")
            print(f"âœ… Table B: {len(self.table_b)} æ¡è®°å½•") 
            print(f"âœ… æœç´¢ç»“æœ: {len(self.search_results)} ä¸ªæŸ¥è¯¢")
        
        # åˆ›å»ºå¿«é€Ÿæ˜ å°„
        self.table_a_dict = self.table_a.set_index('_id').to_dict('index')
        self.table_b_dict = self.table_b.set_index('_id').to_dict('index')
        
    def initialize_reranker(self):
        """åˆå§‹åŒ–é‡æ’åºå™¨"""
        if self.verbose:
            print("âš™ï¸ åˆå§‹åŒ–SimHashé‡æ’åºå™¨...")
        
        self.reranker = SimHashReranker(self.config)
        self.reranker.load_optimization_result('optimization_result.json')
        self.evaluator = RerankerEvaluator()
        
        if self.verbose:
            print(f"ğŸ”§ é…ç½®å‚æ•°: Î±={self.config.alpha} (BM25), Î²={self.config.beta} (SimHash)")
    
    def process_sample_queries(self, sample_size: int = 100):
        """å¤„ç†æ ·æœ¬æŸ¥è¯¢ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•"""
        print(f"\nğŸ¯ å¤„ç†æ ·æœ¬æŸ¥è¯¢ (å‰{sample_size}ä¸ª)...")
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œä¼˜å…ˆé€‰æ‹©æœ‰æ ‡ç­¾çš„æŸ¥è¯¢
        if self.gold_mapping:
            labeled_queries = self.search_results[self.search_results['_id'].isin(self.gold_mapping.keys())]
            if len(labeled_queries) >= sample_size:
                sample_results = labeled_queries.head(sample_size)
                print(f"\u2705 é€‰æ‹©äº† {sample_size} ä¸ªæœ‰çœŸå®æ ‡ç­¾çš„æŸ¥è¯¢")
            else:
                # å…ˆå–æ‰€æœ‰æœ‰æ ‡ç­¾çš„ï¼Œå†è¡¥å……æ— æ ‡ç­¾çš„
                unlabeled_queries = self.search_results[~self.search_results['_id'].isin(self.gold_mapping.keys())]
                remaining = sample_size - len(labeled_queries)
                sample_results = pd.concat([labeled_queries, unlabeled_queries.head(remaining)])
                print(f"\u2705 é€‰æ‹©äº† {len(labeled_queries)} ä¸ªæœ‰æ ‡ç­¾æŸ¥è¯¢ + {remaining} ä¸ªæ— æ ‡ç­¾æŸ¥è¯¢")
        else:
            sample_results = self.search_results.head(sample_size)
            print(f"\u26a0\ufe0f æ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œé€‰æ‹©å‰ {sample_size} ä¸ªæŸ¥è¯¢")
            
        success_count = 0
        total_time = 0
        reranked_results = []
        evaluation_metrics = []
        
        # å¦‚æœæ ·æœ¬å¤§å°è¾ƒå¤§ï¼ˆ>=200ï¼‰ï¼Œä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†
        if sample_size >= 200:
            if self.verbose:
                print(f"  æ ·æœ¬è¾ƒå¤§ï¼Œä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†...")
            
            # å‡†å¤‡å‚æ•°
            optimization_result_path = 'optimization_result.json'
            args_list = [
                (row, self.table_a_dict, self.table_b_dict, self.gold_mapping, 
                 self.config, optimization_result_path)
                for _, row in sample_results.iterrows()
            ]
            
            # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†æ ·æœ¬ï¼ˆä½¿ç”¨imapè·å¾—å®æ—¶è¿›åº¦ï¼‰
            start_time = time.time()
            with mp.Pool(processes=self.num_processes) as pool:
                # ä½¿ç”¨imapæ¥è·å¾—å®æ—¶ç»“æœ
                result_iter = pool.imap(process_single_query_wrapper, args_list)
                
                # å®æ—¶å¤„ç†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
                for idx, result in enumerate(result_iter):
                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯100ä¸ªæ˜¾ç¤ºä¸€æ¬¡ï¼‰
                    if self.verbose and idx % 100 == 0 and idx > 0:
                        progress = idx / len(args_list) * 100
                        elapsed_time = time.time() - start_time
                        print(f"    æ‰§è¡Œè¿›åº¦: {idx}/{len(args_list)} ({progress:.1f}%) | å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
                    
                    if result['success']:
                        success_count += 1
                        total_time += result['processing_time']
                        
                        # è·å–åŸå§‹search_timeä¿¡æ¯
                        original_row = sample_results.iloc[idx]
                        search_time = float(original_row.get('search_time', 0))
                        
                        # æ„å»ºé‡æ’åºç»“æœ
                        reranked_row = {
                            '_id': int(result['query_id']),
                            'ids': [int(x) for x in result['reranked_ids']],
                            'scores': [float(x) for x in result['reranked_scores']],
                            'search_time': search_time,
                            'rerank_time': float(result['processing_time'])
                        }
                        reranked_results.append(reranked_row)
                        
                        # æ”¶é›†è¯„ä¼°æŒ‡æ ‡
                        if result['metrics'] and result['metrics']['has_ground_truth']:
                            evaluation_metrics.append(result['metrics'])
        else:
            # å°æ ·æœ¬ä½¿ç”¨å•çº¿ç¨‹ï¼Œä½†æ·»åŠ è¿›åº¦æ˜¾ç¤º
            start_time = time.time()
            for idx, (_, row) in enumerate(sample_results.iterrows()):
                # æ˜¾ç¤ºè¿›åº¦
                if self.verbose and idx % 50 == 0 and idx > 0:
                    progress = idx / len(sample_results) * 100
                    elapsed_time = time.time() - start_time
                    print(f"    è¿›åº¦: {idx}/{len(sample_results)} ({progress:.1f}%) | å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
                
                result = self._process_single_query(row)
                
                if result['success']:
                    success_count += 1
                    total_time += result['processing_time']
                    
                    # æ„å»ºé‡æ’åºç»“æœ
                    reranked_row = {
                        '_id': int(result['query_id']),
                        'ids': [int(x) for x in result['reranked_ids']],
                        'scores': [float(x) for x in result['reranked_scores']],
                        'search_time': float(row.get('search_time', 0)),
                        'rerank_time': float(result['processing_time'])
                    }
                    reranked_results.append(reranked_row)
                    
                    # æ”¶é›†è¯„ä¼°æŒ‡æ ‡
                    if result['metrics'] and result['metrics']['has_ground_truth']:
                        evaluation_metrics.append(result['metrics'])
        
        print(f"âœ… æ ·æœ¬å¤„ç†å®Œæˆ: {success_count}/{len(sample_results)} æŸ¥è¯¢æˆåŠŸ")
        if self.verbose:
            print(f"â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {total_time / success_count * 1000:.2f} ms")
            print(f"   æœ‰çœŸå®æ ‡ç­¾çš„æŸ¥è¯¢æ•°: {len(evaluation_metrics)}")
        
        # ä¿å­˜æ ·æœ¬ç»“æœ
        if reranked_results:
            reranked_df = pd.DataFrame(reranked_results)
            output_file = f"{self.dataset_name}_sample_reranked_results.parquet"
            reranked_df.to_parquet(output_file)
            if self.verbose:
                print(f"ğŸ’¾ æ ·æœ¬é‡æ’åºç»“æœå·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆæ ·æœ¬è¯„ä¼°æŠ¥å‘Š
        if evaluation_metrics:
            evaluation_report = self._generate_evaluation_report(
                evaluation_metrics, success_count, total_time
            )
            # æ ‡è®°ä¸ºæ ·æœ¬æ¨¡å¼
            evaluation_report['sample_mode'] = True
            evaluation_report['sample_size'] = sample_size
            
            report_file = f"{self.dataset_name}_sample_evaluation_report.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
            if self.verbose:
                print(f"ğŸ“Š æ ·æœ¬è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
            # æ‰“å°æ ·æœ¬æ‘˜è¦
            if self.verbose:
                self.print_sample_summary(evaluation_report)
        
        else:
            if self.verbose:
                print("âš ï¸ æ ·æœ¬ä¸­æ²¡æœ‰æœ‰çœŸå®æ ‡ç­¾çš„æŸ¥è¯¢ï¼Œæ— æ³•ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š")
        
        return reranked_results
    
    def process_all_queries(self):
        """å¤„ç†æ‰€æœ‰æŸ¥è¯¢ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰"""
        print(f"\nğŸš€ å¼€å§‹å¹¶è¡Œå¤„ç†æ‰€æœ‰ {len(self.search_results)} ä¸ªæŸ¥è¯¢...")
        print(f"   ä½¿ç”¨ {self.num_processes} ä¸ªè¿›ç¨‹")
        
        # å‡†å¤‡å‚æ•°
        optimization_result_path = 'optimization_result.json'
        
        # åˆ›å»ºå‚æ•°åˆ—è¡¨
        args_list = [
            (row, self.table_a_dict, self.table_b_dict, self.gold_mapping, 
             self.config, optimization_result_path)
            for _, row in self.search_results.iterrows()
        ]
        
        # ä½¿ç”¨å¤šè¿›ç¨‹å¤„ç†ï¼ˆä½¿ç”¨imapè·å¾—å®æ—¶è¿›åº¦ï¼‰
        start_time = time.time()
        reranked_results = []
        evaluation_metrics = []
        success_count = 0
        total_rerank_time = 0
        
        with mp.Pool(processes=self.num_processes) as pool:
            # ä½¿ç”¨imapæ¥è·å¾—å®æ—¶ç»“æœ
            result_iter = pool.imap(process_single_query_wrapper, args_list)
            
            # å®æ—¶å¤„ç†ç»“æœå¹¶æ˜¾ç¤ºè¿›åº¦
            for idx, result in enumerate(result_iter):
                # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯1000ä¸ªæ˜¾ç¤ºä¸€æ¬¡ï¼Œå› ä¸ºå…¨é‡æ•°æ®æ›´å¤šï¼‰
                if self.verbose and idx % 1000 == 0 and idx > 0:
                    progress = idx / len(args_list) * 100
                    elapsed_time = time.time() - start_time
                    print(f"  æ‰§è¡Œè¿›åº¦: {idx}/{len(args_list)} ({progress:.1f}%) | å·²ç”¨æ—¶: {elapsed_time:.1f}ç§’")
                
                if result['success']:
                    success_count += 1
                    total_rerank_time += result['processing_time']
                    
                    # æ„å»ºé‡æ’åºç»“æœ
                    reranked_row = {
                        '_id': int(result['query_id']),
                        'ids': [int(x) for x in result['reranked_ids']],
                        'scores': [float(x) for x in result['reranked_scores']],
                        'search_time': 0,  # åŸå§‹æ•°æ®ä¸­å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                        'rerank_time': float(result['processing_time'])
                    }
                    reranked_results.append(reranked_row)
                    
                    # è¯„ä¼°æŒ‡æ ‡
                    if result['metrics'] and result['metrics']['has_ground_truth']:
                        evaluation_metrics.append(result['metrics'])
        
        total_time = time.time() - start_time
        
        print(f"âœ… å¤„ç†å®Œæˆ: {success_count}/{len(self.search_results)} æŸ¥è¯¢æˆåŠŸ | æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        # ä¿å­˜é‡æ’åºç»“æœ
        if reranked_results:
            reranked_df = pd.DataFrame(reranked_results)
            output_file = f"{self.dataset_name}_reranked_results_k50.parquet"
            reranked_df.to_parquet(output_file)
            if self.verbose:
                print(f"ğŸ’¾ é‡æ’åºç»“æœå·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = self._generate_evaluation_report(
            evaluation_metrics, success_count, total_rerank_time
        )
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        report_file = f"{self.dataset_name}_evaluation_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
        if self.verbose:
            print(f"ğŸ“Š è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return evaluation_report
    
    def _process_single_query(self, row: pd.Series) -> Dict:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢ï¼ˆç”¨äºæ ·æœ¬å¤„ç†ï¼‰"""
        query_id = row['_id']
        candidate_ids = row['ids']
        original_scores = row['scores']
        
        # è·å–æŸ¥è¯¢è®°å½•
        if query_id not in self.table_b_dict:
            return {'success': False, 'error': f'Query ID {query_id} not found'}
            
        query_record = self.table_b_dict[query_id]
        
        # è·å–å€™é€‰è®°å½•
        candidate_records = []
        valid_candidates = []
        valid_scores = []
        
        for i, cand_id in enumerate(candidate_ids):
            if cand_id in self.table_a_dict:
                candidate_records.append(self.table_a_dict[cand_id])
                valid_candidates.append(cand_id)
                valid_scores.append(original_scores[i])
        
        if not candidate_records:
            return {'success': False, 'error': 'No valid candidates'}
        
        # æ‰§è¡Œé‡æ’åº
        try:
            ranking_indices, fused_scores, debug_info = self.reranker.rerank_candidates(
                query_record, candidate_records, valid_scores
            )
            
            reranked_ids = [valid_candidates[i] for i in ranking_indices]
            
            # è¯„ä¼°æŒ‡æ ‡
            metrics = None
            if query_id in self.gold_mapping:
                metrics = self.evaluator.calculate_metrics(
                    query_id, valid_candidates, reranked_ids, self.gold_mapping
                )
            
            return {
                'success': True,
                'query_id': query_id,
                'reranked_ids': reranked_ids,
                'reranked_scores': fused_scores,
                'processing_time': debug_info['processing_time'],
                'metrics': metrics
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    

    
    def _create_simple_case(self, row: pd.Series, result: Dict) -> Dict:
        """åˆ›å»ºç®€åŒ–æ¡ˆä¾‹"""
        query_id = result['query_id']
        query_record = self.table_b_dict[query_id]
        
        # è½¬æ¢metricsä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
        metrics = result['metrics']
        clean_metrics = {}
        for k, v in metrics.items():
            if isinstance(v, np.integer):
                clean_metrics[k] = int(v)
            elif isinstance(v, np.floating):
                clean_metrics[k] = float(v)
            elif isinstance(v, (int, float)):
                clean_metrics[k] = v
            else:
                clean_metrics[k] = v
        
        case = {
            'query_id': int(query_id),
            'query_info': {k: str(v) for k, v in query_record.items() if k != '_id'},
            'metrics': clean_metrics,
            'processing_time': float(result['processing_time'])
        }
        
        return case
    
    def _generate_evaluation_report(self, metrics_list: List[Dict], 
                                  success_count: int, total_rerank_time: float) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            'dataset': self.dataset_name,
            'total_queries': len(self.search_results),
            'successful_queries': success_count,
            'queries_with_ground_truth': len(metrics_list),
            'total_rerank_time': total_rerank_time,
            'avg_rerank_time': total_rerank_time / success_count if success_count > 0 else 0,
            'config': self.config.__dict__,
            'evaluation_available': len(metrics_list) > 0
        }
        
        if metrics_list:
            # è®¡ç®—å„é¡¹æŒ‡æ ‡
            for k in [1, 5, 10, 20]:
                orig_recalls = [m[f'original_recall@{k}'] for m in metrics_list]
                rerank_recalls = [m[f'reranked_recall@{k}'] for m in metrics_list]
                rank_improvements = [m[f'rank_improvement@{k}'] for m in metrics_list]
                
                report[f'avg_original_recall@{k}'] = float(np.mean(orig_recalls))
                report[f'avg_reranked_recall@{k}'] = float(np.mean(rerank_recalls))
                report[f'recall_improvement@{k}'] = float(np.mean(rerank_recalls) - np.mean(orig_recalls))
                report[f'avg_rank_improvement@{k}'] = float(np.mean(rank_improvements))
                report[f'positive_improvements@{k}'] = int(sum(1 for x in rank_improvements if x > 0))
                report[f'negative_improvements@{k}'] = int(sum(1 for x in rank_improvements if x < 0))
        
        return report
    
    def print_summary(self, evaluation_report: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ† {self.dataset_name} æ•°æ®é›†é‡æ’åºè¯„ä¼°æ‘˜è¦")
        print(f"{'='*60}")
        
        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æŸ¥è¯¢æ•°: {evaluation_report['total_queries']}")
        print(f"   æˆåŠŸå¤„ç†: {evaluation_report['successful_queries']}")
        print(f"   æœ‰çœŸå®æ ‡ç­¾: {evaluation_report['queries_with_ground_truth']}")
        print(f"   å¹³å‡é‡æ’åºæ—¶é—´: {evaluation_report['avg_rerank_time'] * 1000:.2f} ms")
        print(f"   æ€»å¤„ç†æ—¶é—´: {evaluation_report['total_rerank_time']:.2f} ç§’")
        
        if evaluation_report['evaluation_available']:
            print("\nğŸ“ˆ æ€§èƒ½æ”¹å–„:")
            for k in [1, 5, 10]:
                improvement = evaluation_report[f'recall_improvement@{k}']
                positive = evaluation_report[f'positive_improvements@{k}']
                negative = evaluation_report[f'negative_improvements@{k}']
                total = evaluation_report['queries_with_ground_truth']
                
                status = "ğŸŸ¢" if improvement > 0 else "ğŸ”´" if improvement < 0 else "ğŸŸ¡"
                print(f"   {status} Recall@{k}: {improvement:+.4f} ({positive}â†‘/{negative}â†“/{total} æŸ¥è¯¢)")
        
        print("\nğŸ’¾ è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“„ {self.dataset_name}_reranked_results_k50.parquet")
        print(f"   ğŸ“Š {self.dataset_name}_evaluation_report.json")
    
    def print_sample_summary(self, evaluation_report: Dict):
        """æ‰“å°æ ·æœ¬è¯„ä¼°æ‘˜è¦"""
        print(f"\n{'='*50}")
        print(f"ğŸ† {self.dataset_name} æ ·æœ¬è¯„ä¼°æ‘˜è¦")
        print(f"{'='*50}")
        
        print(f"ğŸ“Š æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ ·æœ¬å¤§å°: {evaluation_report['sample_size']}")
        print(f"   æˆåŠŸå¤„ç†: {evaluation_report['successful_queries']}")
        print(f"   æœ‰çœŸå®æ ‡ç­¾: {evaluation_report['queries_with_ground_truth']}")
        print(f"   å¹³å‡é‡æ’åºæ—¶é—´: {evaluation_report['avg_rerank_time'] * 1000:.2f} ms")
        
        if evaluation_report['evaluation_available']:
            print("\nğŸ“ˆ æ€§èƒ½æ”¹å–„:")
            for k in [1, 5, 10]:
                improvement = evaluation_report[f'recall_improvement@{k}']
                positive = evaluation_report[f'positive_improvements@{k}']
                negative = evaluation_report[f'negative_improvements@{k}']
                total = evaluation_report['queries_with_ground_truth']
                
                status = "ğŸŸ¢" if improvement > 0 else "ğŸ”´" if improvement < 0 else "ğŸŸ¡"
                print(f"   {status} Recall@{k}: {improvement:+.4f} ({positive}â†‘/{negative}â†“/{total} æŸ¥è¯¢)")
        
        print("\nğŸ’¾ æ ·æœ¬è¾“å‡ºæ–‡ä»¶:")
        print(f"   ğŸ“„ {self.dataset_name}_sample_reranked_results.parquet")
        print(f"   ğŸ“Š {self.dataset_name}_sample_evaluation_report.json")
        print(f"   ğŸ“ {self.dataset_name}_sample_cases.json")


def main():
    parser = argparse.ArgumentParser(description='é€šç”¨æ•°æ®é›†é‡æ’åºè¯¦ç»†è¯„ä¼°ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰')
    parser.add_argument('--alpha', type=float, default=0.6,
                       help='BM25æƒé‡ (é»˜è®¤: 0.6)')
    parser.add_argument('--beta', type=float, default=0.4,
                       help='SimHashæƒé‡ (é»˜è®¤: 0.4)')
    parser.add_argument('--simhash_bits', type=int, default=64,
                       help='SimHashä½æ•° (é»˜è®¤: 64)')
    parser.add_argument('--sample_only', action='store_true',
                       help='åªå¤„ç†æ ·æœ¬æŸ¥è¯¢ï¼Œç”¨äºå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--sample_size', type=int, default=100,
                       help='æ ·æœ¬å¤§å° (é»˜è®¤: 100)')
    parser.add_argument('--processes', type=int, default=mp.cpu_count(),
                       help='å¹¶è¡Œè¿›ç¨‹æ•° (é»˜è®¤: CPUæ ¸å¿ƒæ•°)')
    parser.add_argument('--quiet', action='store_true',
                       help='é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡ºä¿¡æ¯')
    
    args = parser.parse_args()
    
    # è‡ªåŠ¨æ£€æµ‹æ•°æ®é›†åç§°
    dataset_name = Path.cwd().name
    if not args.quiet:
        print(f"ğŸŒŸ {dataset_name} æ•°æ®é›†SimHashé‡æ’åºè¯„ä¼°ï¼ˆå¹¶è¡Œç‰ˆæœ¬ï¼‰")
        print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = RerankerConfig(
        simhash_bits=args.simhash_bits,
        alpha=args.alpha,
        beta=args.beta,
        use_3gram=True,
        normalize_scores=True
    )
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = UniversalDatasetEvaluatorParallel(config, args.processes, verbose=not args.quiet)
    
    try:
        # åŠ è½½æ•°æ®
        evaluator.load_data()

        # åˆå§‹åŒ–é‡æ’åºå™¨
        evaluator.initialize_reranker()
        
        # å¤„ç†æŸ¥è¯¢
        if args.sample_only:
            if not args.quiet:
                print(f"\nğŸ¯ æ ·æœ¬æ¨¡å¼ï¼šå¤„ç†å‰ {args.sample_size} ä¸ªæŸ¥è¯¢")
            evaluator.process_sample_queries(args.sample_size)
        else:
            evaluation_report = evaluator.process_all_queries()
            if not args.quiet:
                evaluator.print_summary(evaluation_report)
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()