#!/usr/bin/env python3
"""
æ‰¹é‡é‡æŽ’åºè„šæœ¬ - å¤„ç†å®Œæ•´çš„parquetæ•°æ®é›†

è¾“å…¥æ–‡ä»¶:
- sparkly_results_k50.parquet: Sparklyçš„å®Œæ•´æœç´¢ç»“æžœ
- table_a.parquet: è¢«ç´¢å¼•çš„å®Œæ•´æ•°æ®
- table_b.parquet: æŸ¥è¯¢çš„å®Œæ•´æ•°æ®
- optimization_result.json: å­—æ®µæƒé‡é…ç½®
- gold.parquet: çœŸå®žæ ‡ç­¾(å¯é€‰ï¼Œç”¨äºŽè¯„ä¼°)

è¾“å‡ºæ–‡ä»¶:
- reranked_results_k50.parquet: é‡æŽ’åºåŽçš„å®Œæ•´ç»“æžœ
- rerank_evaluation_report.json: æ€§èƒ½è¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
python batch_rerank_parquet.py [--config_file optimization_result.json] [--output_file reranked_results_k50.parquet]
"""

import pandas as pd
import numpy as np
import json
import time
import argparse
from typing import Dict, List, Tuple, Optional
from simhash_reranker import SimHashReranker, RerankerEvaluator, RerankerConfig


class ParquetBatchReranker:
    """å®Œæ•´parquetæ•°æ®é›†çš„æ‰¹é‡é‡æŽ’åºå™¨"""
    
    def __init__(self, config: RerankerConfig = None):
        self.config = config or RerankerConfig(
            simhash_bits=64,
            alpha=0.6,      # ä½¿ç”¨æœ€ä½³å‚æ•°
            beta=0.4,
            use_3gram=True,
            normalize_scores=True
        )
        self.reranker = None
        self.evaluator = RerankerEvaluator()
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰parquetæ•°æ®"""
        print("ðŸ“ åŠ è½½å®Œæ•´æ•°æ®é›†...")
        
        # åŠ è½½è¡¨æ ¼æ•°æ®
        self.table_a = pd.read_parquet('table_a.parquet')
        self.table_b = pd.read_parquet('table_b.parquet')
        
        # åŠ è½½Sparklyæœç´¢ç»“æžœ
        self.search_results = pd.read_parquet('sparkly_results_k50.parquet')
        
        # å°è¯•åŠ è½½çœŸå®žæ ‡ç­¾(ç”¨äºŽè¯„ä¼°)
        try:
            self.gold_df = pd.read_parquet('gold.parquet')
            self.gold_mapping = dict(zip(self.gold_df['id2'], self.gold_df['id1']))
            print(f"âœ… åŠ è½½äº† {len(self.gold_mapping)} ä¸ªçœŸå®žæ ‡ç­¾")
        except FileNotFoundError:
            print("âš ï¸ æœªæ‰¾åˆ°gold.parquetæ–‡ä»¶ï¼Œå°†è·³è¿‡æ€§èƒ½è¯„ä¼°")
            self.gold_df = None
            self.gold_mapping = {}
        
        print(f"âœ… Table A: {len(self.table_a)} æ¡è®°å½•")
        print(f"âœ… Table B: {len(self.table_b)} æ¡è®°å½•")
        print(f"âœ… æœç´¢ç»“æžœ: {len(self.search_results)} ä¸ªæŸ¥è¯¢")
        
        # åˆ›å»ºIDåˆ°è®°å½•çš„å¿«é€Ÿæ˜ å°„
        self.table_a_dict = self.table_a.set_index('_id').to_dict('index')
        self.table_b_dict = self.table_b.set_index('_id').to_dict('index')
        
    def initialize_reranker(self, config_file: str = 'optimization_result.json'):
        """åˆå§‹åŒ–é‡æŽ’åºå™¨"""
        print("âš™ï¸ åˆå§‹åŒ–é‡æŽ’åºå™¨...")
        self.reranker = SimHashReranker(self.config)
        self.reranker.load_optimization_result(config_file)
        print("âœ… é‡æŽ’åºå™¨åˆå§‹åŒ–å®Œæˆ")
        
    def process_single_query(self, row: pd.Series) -> Dict:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢çš„é‡æŽ’åº"""
        query_id = row['_id']
        candidate_ids = row['ids']
        original_scores = row['scores']
        
        # èŽ·å–æŸ¥è¯¢è®°å½•
        if query_id not in self.table_b_dict:
            return {
                'query_id': query_id,
                'success': False,
                'error': f'Query ID {query_id} not found in table_b'
            }
            
        query_record = self.table_b_dict[query_id]
        
        # èŽ·å–å€™é€‰è®°å½•
        candidate_records = []
        valid_candidates = []
        valid_scores = []
        
        for i, cand_id in enumerate(candidate_ids):
            if cand_id in self.table_a_dict:
                candidate_records.append(self.table_a_dict[cand_id])
                valid_candidates.append(cand_id)
                valid_scores.append(original_scores[i])
        
        if not candidate_records:
            return {
                'query_id': query_id,
                'success': False,
                'error': 'No valid candidates found'
            }
        
        # æ‰§è¡Œé‡æŽ’åº
        try:
            ranking_indices, fused_scores, debug_info = self.reranker.rerank_candidates(
                query_record, candidate_records, valid_scores
            )
            
            # é‡æ–°æŽ’åˆ—ç»“æžœ
            reranked_ids = [valid_candidates[i] for i in ranking_indices]
            reranked_scores = [fused_scores[i] for i in range(len(fused_scores))]
            
            return {
                'query_id': query_id,
                'success': True,
                'original_ids': valid_candidates,
                'original_scores': valid_scores,
                'reranked_ids': reranked_ids,
                'reranked_scores': reranked_scores,
                'processing_time': debug_info['processing_time']
            }
            
        except Exception as e:
            return {
                'query_id': query_id,
                'success': False,
                'error': str(e)
            }
    
    def batch_rerank(self) -> Tuple[pd.DataFrame, Dict]:
        """æ‰¹é‡é‡æŽ’åºæ‰€æœ‰æŸ¥è¯¢"""
        print("ðŸ”„ å¼€å§‹æ‰¹é‡é‡æŽ’åº...")
        
        reranked_results = []
        evaluation_metrics = []
        total_time = 0
        success_count = 0
        
        for idx, row in self.search_results.iterrows():
            if idx % 100 == 0:
                print(f"è¿›åº¦: {idx}/{len(self.search_results)} ({idx/len(self.search_results)*100:.1f}%)")
            
            result = self.process_single_query(row)
            
            if result['success']:
                success_count += 1
                total_time += result['processing_time']
                
                # æž„å»ºé‡æŽ’åºç»“æžœè¡Œ
                reranked_row = {
                    '_id': result['query_id'],
                    'ids': result['reranked_ids'],
                    'scores': result['reranked_scores'],
                    'search_time': row['search_time'],  # ä¿æŒåŽŸå§‹æœç´¢æ—¶é—´
                    'rerank_time': result['processing_time']
                }
                reranked_results.append(reranked_row)
                
                # å¦‚æžœæœ‰çœŸå®žæ ‡ç­¾ï¼Œè®¡ç®—è¯„ä¼°æŒ‡æ ‡
                if result['query_id'] in self.gold_mapping:
                    metrics = self.evaluator.calculate_metrics(
                        result['query_id'],
                        result['original_ids'],
                        result['reranked_ids'], 
                        self.gold_mapping
                    )
                    evaluation_metrics.append(metrics)
            else:
                print(f"âš ï¸ æŸ¥è¯¢ {result['query_id']} å¤„ç†å¤±è´¥: {result['error']}")
        
        # åˆ›å»ºç»“æžœDataFrame
        reranked_df = pd.DataFrame(reranked_results)
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = self._generate_evaluation_report(
            evaluation_metrics, success_count, total_time
        )
        
        print(f"\nâœ… æ‰¹é‡é‡æŽ’åºå®Œæˆ!")
        print(f"æˆåŠŸå¤„ç†: {success_count}/{len(self.search_results)} ä¸ªæŸ¥è¯¢")
        print(f"æ€»å¤„ç†æ—¶é—´: {total_time:.4f} ç§’")
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {total_time/success_count:.4f} ç§’/æŸ¥è¯¢")
        
        return reranked_df, evaluation_report
    
    def _generate_evaluation_report(self, metrics_list: List[Dict], 
                                  success_count: int, total_time: float) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        if not metrics_list:
            return {
                'total_queries_processed': success_count,
                'total_processing_time': total_time,
                'avg_processing_time': total_time / success_count if success_count > 0 else 0,
                'evaluation_available': False,
                'message': 'æ²¡æœ‰å¯ç”¨çš„çœŸå®žæ ‡ç­¾è¿›è¡Œæ€§èƒ½è¯„ä¼°'
            }
        
        # æ±‡æ€»è¯„ä¼°æŒ‡æ ‡
        report = {
            'total_queries_processed': success_count,
            'queries_with_ground_truth': len(metrics_list),
            'total_processing_time': total_time,
            'avg_processing_time': total_time / success_count if success_count > 0 else 0,
            'evaluation_available': True,
            'config': self.config.__dict__
        }
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡çš„å¹³å‡å€¼
        for k in [1, 5, 10, 20]:
            if f'original_recall@{k}' in metrics_list[0]:
                orig_recalls = [m[f'original_recall@{k}'] for m in metrics_list]
                rerank_recalls = [m[f'reranked_recall@{k}'] for m in metrics_list]
                rank_improvements = [m[f'rank_improvement@{k}'] for m in metrics_list]
                
                report[f'avg_original_recall@{k}'] = np.mean(orig_recalls)
                report[f'avg_reranked_recall@{k}'] = np.mean(rerank_recalls)
                report[f'recall_improvement@{k}'] = np.mean(rerank_recalls) - np.mean(orig_recalls)
                report[f'avg_rank_improvement@{k}'] = np.mean(rank_improvements)
                report[f'positive_improvements@{k}'] = sum(1 for x in rank_improvements if x > 0)
                report[f'negative_improvements@{k}'] = sum(1 for x in rank_improvements if x < 0)
        
        return report
    
    def save_results(self, reranked_df: pd.DataFrame, 
                    evaluation_report: Dict, output_file: str):
        """ä¿å­˜é‡æŽ’åºç»“æžœå’Œè¯„ä¼°æŠ¥å‘Š"""
        # ä¿å­˜é‡æŽ’åºåŽçš„parquetæ–‡ä»¶
        reranked_df.to_parquet(output_file)
        print(f"ðŸ’¾ é‡æŽ’åºç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        
        # ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        report_file = output_file.replace('.parquet', '_evaluation_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_report, f, indent=2, ensure_ascii=False)
        print(f"ðŸ“Š è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ‰“å°æ€§èƒ½æ‘˜è¦
        if evaluation_report['evaluation_available']:
            print(f"\nðŸ“ˆ æ€§èƒ½æ”¹å–„æ‘˜è¦:")
            for k in [1, 5, 10]:
                improvement = evaluation_report[f'recall_improvement@{k}']
                positive = evaluation_report[f'positive_improvements@{k}']
                total = evaluation_report['queries_with_ground_truth']
                print(f"   Recall@{k}: {improvement:+.4f} ({positive}/{total} æŸ¥è¯¢æ”¹å–„)")


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡é‡æŽ’åºå®Œæ•´parquetæ•°æ®é›†')
    parser.add_argument('--config_file', type=str, default='optimization_result.json',
                       help='å­—æ®µæƒé‡é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_file', type=str, default='reranked_results_k50.parquet',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--alpha', type=float, default=0.6,
                       help='BM25æƒé‡ (é»˜è®¤: 0.6)')
    parser.add_argument('--beta', type=float, default=0.4,
                       help='SimHashæƒé‡ (é»˜è®¤: 0.4)')
    parser.add_argument('--simhash_bits', type=int, default=64,
                       help='SimHashä½æ•° (é»˜è®¤: 64)')
    
    args = parser.parse_args()
    
    print("ðŸš€ å¼€å§‹æ‰¹é‡é‡æŽ’åºå®Œæ•´parquetæ•°æ®é›†")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = RerankerConfig(
        simhash_bits=args.simhash_bits,
        alpha=args.alpha,
        beta=args.beta,
        use_3gram=True,
        normalize_scores=True
    )
    
    print(f"ðŸ”§ é…ç½®å‚æ•°: Î±={config.alpha}, Î²={config.beta}, bits={config.simhash_bits}")
    
    # åˆ›å»ºé‡æŽ’åºå™¨
    reranker = ParquetBatchReranker(config)
    
    # åŠ è½½æ•°æ®
    reranker.load_data()
    
    # åˆå§‹åŒ–é‡æŽ’åºå™¨
    reranker.initialize_reranker(args.config_file)
    
    # æ‰§è¡Œæ‰¹é‡é‡æŽ’åº
    reranked_df, evaluation_report = reranker.batch_rerank()
    
    # ä¿å­˜ç»“æžœ
    reranker.save_results(reranked_df, evaluation_report, args.output_file)
    
    print("\nðŸŽŠ æ‰¹é‡é‡æŽ’åºå®Œæˆ!")
    print(f"ðŸ“„ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - é‡æŽ’åºç»“æžœ: {args.output_file}")
    print(f"   - è¯„ä¼°æŠ¥å‘Š: {args.output_file.replace('.parquet', '_evaluation_report.json')}")


if __name__ == "__main__":
    main() 