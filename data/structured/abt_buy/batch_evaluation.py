#!/usr/bin/env python3
"""
æ‰¹é‡è¯„ä¼°è„šæœ¬ - å¯¹æ•´ä¸ªabt_buyæ•°æ®é›†è¿›è¡ŒSimHashé‡æ’åºè¯„ä¼°

åŠŸèƒ½:
1. æ‰¹é‡å¤„ç†æ‰€æœ‰æŸ¥è¯¢
2. ç»Ÿè®¡ä¸åŒå‚æ•°ä¸‹çš„æ€§èƒ½è¡¨ç°
3. ç”Ÿæˆè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
4. å‚æ•°ä¼˜åŒ–å»ºè®®

è¿è¡Œæ–¹å¼: python batch_evaluation.py
"""

import json
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import time
from itertools import product
import matplotlib.pyplot as plt
import seaborn as sns
from simhash_reranker import SimHashReranker, RerankerEvaluator, RerankerConfig


class BatchEvaluator:
    """æ‰¹é‡è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.table_a = None
        self.table_b = None
        self.gold_mapping = None
        self.evaluator = RerankerEvaluator()
        
    def load_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("ğŸ“ åŠ è½½æ•°æ®é›†...")
        
        # åŠ è½½è¡¨æ ¼æ•°æ®
        self.table_a = self._load_table('table_a.md')
        self.table_b = self._load_table('table_b.md')
        
        # åŠ è½½çœŸå®æ ‡ç­¾
        self.gold_mapping = self.evaluator.load_ground_truth('gold_part.md')
        
        print(f"âœ… Table A: {len(self.table_a)} æ¡è®°å½•")
        print(f"âœ… Table B: {len(self.table_b)} æ¡è®°å½•")
        print(f"âœ… çœŸå®æ ‡ç­¾: {len(self.gold_mapping)} å¯¹")
        
    def _load_table(self, table_path: str) -> pd.DataFrame:
        """åŠ è½½è¡¨æ ¼æ•°æ®"""
        records = []
        with open(table_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    records.append(json.loads(line.strip()))
        return pd.DataFrame(records)
    
    def simulate_search_results(self, k: int = 50) -> Dict[int, Dict]:
        """æ¨¡æ‹Ÿæœç´¢ç»“æœ - ä¸ºæ‰€æœ‰æœ‰çœŸå®æ ‡ç­¾çš„æŸ¥è¯¢ç”Ÿæˆå€™é€‰"""
        print(f"ğŸ” æ¨¡æ‹Ÿæœç´¢ç»“æœ (K={k})...")
        
        search_results = {}
        
        for query_id in self.gold_mapping.keys():
            if query_id >= len(self.table_b):
                continue
                
            # è·å–æŸ¥è¯¢è®°å½•
            query_record = self.table_b.iloc[query_id]
            query_text = f"{query_record['name']} {query_record['description']}".lower()
            
            # ç®€å•çš„åŸºäºè¯æ±‡é‡å çš„ç›¸ä¼¼åº¦è®¡ç®— (æ¨¡æ‹ŸBM25)
            candidate_scores = []
            
            for _, candidate in self.table_a.iterrows():
                candidate_text = f"{candidate['name']} {candidate['description']}".lower()
                
                # è®¡ç®—ç®€å•çš„è¯æ±‡é‡å åˆ†æ•°
                query_words = set(query_text.split())
                candidate_words = set(candidate_text.split())
                
                if len(query_words) == 0:
                    score = 0.0
                else:
                    overlap = len(query_words.intersection(candidate_words))
                    score = overlap / len(query_words) + np.random.normal(0, 0.1)  # æ·»åŠ å™ªéŸ³
                
                candidate_scores.append((candidate['_id'], score))
            
            # æ’åºå¹¶å–å‰Kä¸ª
            candidate_scores.sort(key=lambda x: x[1], reverse=True)
            top_k = candidate_scores[:k]
            
            search_results[query_id] = {
                'ids': [item[0] for item in top_k],
                'scores': [item[1] for item in top_k]
            }
        
        return search_results
    
    def evaluate_configuration(self, 
                             config: RerankerConfig, 
                             search_results: Dict[int, Dict]) -> Dict:
        """è¯„ä¼°ç‰¹å®šé…ç½®çš„æ€§èƒ½"""
        
        # åˆ›å»ºé‡æ’åºå™¨
        reranker = SimHashReranker(config)
        reranker.load_optimization_result('optimization_result.json')
        
        # æ”¶é›†æ‰€æœ‰æŸ¥è¯¢çš„è¯„ä¼°ç»“æœ
        all_metrics = []
        total_processing_time = 0
        
        for query_id, result in search_results.items():
            if query_id not in self.gold_mapping:
                continue
                
            # è·å–æŸ¥è¯¢å’Œå€™é€‰è®°å½•
            query_record = self.table_b.iloc[query_id].to_dict()
            
            candidate_records = []
            for cand_id in result['ids']:
                candidate_record = self.table_a[self.table_a['_id'] == cand_id].iloc[0].to_dict()
                candidate_records.append(candidate_record)
            
            # æ‰§è¡Œé‡æ’åº
            start_time = time.time()
            ranking_indices, fused_scores, debug_info = reranker.rerank_candidates(
                query_record, candidate_records, result['scores']
            )
            total_processing_time += time.time() - start_time
            
            # é‡æ–°æ’åˆ—å€™é€‰ID
            reranked_ids = [result['ids'][i] for i in ranking_indices]
            
            # è¯„ä¼°æ•ˆæœ
            metrics = self.evaluator.calculate_metrics(
                query_id, result['ids'], reranked_ids, self.gold_mapping
            )
            
            if metrics['has_ground_truth']:
                all_metrics.append(metrics)
        
        # æ±‡æ€»ç»Ÿè®¡
        summary = self._aggregate_metrics(all_metrics)
        summary['config'] = config.__dict__
        summary['total_queries'] = len(all_metrics)
        summary['avg_processing_time'] = total_processing_time / len(all_metrics) if all_metrics else 0
        
        return summary
    
    def _aggregate_metrics(self, all_metrics: List[Dict]) -> Dict:
        """æ±‡æ€»è¯„ä¼°æŒ‡æ ‡"""
        if not all_metrics:
            return {}
        
        summary = {}
        
        # è®¡ç®—å„ä¸ªKå€¼ä¸‹çš„å¹³å‡æŒ‡æ ‡
        for k in [1, 5, 10, 20]:
            # Recall@K
            original_recalls = [m[f'original_recall@{k}'] for m in all_metrics]
            reranked_recalls = [m[f'reranked_recall@{k}'] for m in all_metrics]
            
            summary[f'avg_original_recall@{k}'] = np.mean(original_recalls)
            summary[f'avg_reranked_recall@{k}'] = np.mean(reranked_recalls)
            summary[f'recall_improvement@{k}'] = np.mean(reranked_recalls) - np.mean(original_recalls)
            
            # æ’åæ”¹å–„
            rank_improvements = [m[f'rank_improvement@{k}'] for m in all_metrics]
            summary[f'avg_rank_improvement@{k}'] = np.mean(rank_improvements)
            summary[f'positive_rank_improvements@{k}'] = sum(1 for x in rank_improvements if x > 0)
            summary[f'negative_rank_improvements@{k}'] = sum(1 for x in rank_improvements if x < 0)
        
        return summary
    
    def grid_search_parameters(self, search_results: Dict[int, Dict]) -> pd.DataFrame:
        """ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°"""
        print("ğŸ”§ ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°...")
        
        # å‚æ•°ç½‘æ ¼
        alpha_values = [0.5, 0.6, 0.7, 0.8, 0.9]
        beta_values = [0.1, 0.2, 0.3, 0.4, 0.5]
        simhash_bits = [32, 64]
        
        results = []
        total_combinations = len(alpha_values) * len(beta_values) * len(simhash_bits)
        
        print(f"æ€»å…±éœ€è¦æµ‹è¯• {total_combinations} ç§å‚æ•°ç»„åˆ...")
        
        for i, (alpha, beta, bits) in enumerate(product(alpha_values, beta_values, simhash_bits)):
            # ç¡®ä¿alpha + beta <= 1.0
            if alpha + beta > 1.0:
                continue
                
            print(f"è¿›åº¦: {i+1}/{total_combinations} - alpha={alpha}, beta={beta}, bits={bits}")
            
            config = RerankerConfig(
                simhash_bits=bits,
                alpha=alpha,
                beta=beta,
                use_3gram=True,
                normalize_scores=True
            )
            
            try:
                summary = self.evaluate_configuration(config, search_results)
                summary['alpha'] = alpha
                summary['beta'] = beta
                summary['simhash_bits'] = bits
                results.append(summary)
            except Exception as e:
                print(f"å‚æ•°ç»„åˆå¤±è´¥: {e}")
                continue
        
        return pd.DataFrame(results)
    
    def generate_report(self, results_df: pd.DataFrame):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š...")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_df.to_csv('parameter_optimization_results.csv', index=False)
        
        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_config = results_df.loc[results_df['recall_improvement@10'].idxmax()]
        
        print("\n" + "="*60)
        print("ğŸ† æœ€ä½³é…ç½®:")
        print(f"   Alpha (BM25æƒé‡): {best_config['alpha']}")
        print(f"   Beta (SimHashæƒé‡): {best_config['beta']}")
        print(f"   SimHashä½æ•°: {best_config['simhash_bits']}")
        print(f"   Recall@10æå‡: {best_config['recall_improvement@10']:.4f}")
        print(f"   å¹³å‡å¤„ç†æ—¶é—´: {best_config['avg_processing_time']:.4f} ç§’")
        
        # æ˜¾ç¤ºä¸åŒKå€¼ä¸‹çš„æ”¹å–„æƒ…å†µ
        print("\nğŸ“ˆ æœ€ä½³é…ç½®ä¸‹çš„æ€§èƒ½æå‡:")
        for k in [1, 5, 10, 20]:
            improvement = best_config[f'recall_improvement@{k}']
            positive = best_config[f'positive_rank_improvements@{k}']
            negative = best_config[f'negative_rank_improvements@{k}']
            total = best_config['total_queries']
            
            print(f"   Recall@{k}: +{improvement:.4f} ({positive}/{total} æŸ¥è¯¢æ”¹å–„, {negative}/{total} æŸ¥è¯¢ä¸‹é™)")
        
        # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        self._create_visualizations(results_df)
    
    def _create_visualizations(self, results_df: pd.DataFrame):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        try:
            plt.style.use('seaborn-v0_8')
        except:
            plt.style.use('default')
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Alpha vs Recall@10 æ”¹å–„
        pivot_alpha = results_df.pivot_table(
            values='recall_improvement@10', 
            index='alpha', 
            columns='beta', 
            aggfunc='mean'
        )
        
        sns.heatmap(pivot_alpha, annot=True, fmt='.4f', cmap='RdYlGn', 
                   center=0, ax=axes[0,0])
        axes[0,0].set_title('Recall@10æ”¹å–„ vs Alpha/Betaå‚æ•°')
        
        # 2. SimHashä½æ•°å¯¹æ¯”
        bit_comparison = results_df.groupby('simhash_bits').agg({
            'recall_improvement@10': 'mean',
            'avg_processing_time': 'mean'
        }).reset_index()
        
        axes[0,1].bar(bit_comparison['simhash_bits'], bit_comparison['recall_improvement@10'])
        axes[0,1].set_title('ä¸åŒSimHashä½æ•°çš„Recall@10æ”¹å–„')
        axes[0,1].set_xlabel('SimHashä½æ•°')
        axes[0,1].set_ylabel('å¹³å‡Recall@10æ”¹å–„')
        
        # 3. å¤„ç†æ—¶é—´ vs æ€§èƒ½
        axes[1,0].scatter(results_df['avg_processing_time'], results_df['recall_improvement@10'],
                         alpha=0.6)
        axes[1,0].set_xlabel('å¹³å‡å¤„ç†æ—¶é—´ (ç§’)')
        axes[1,0].set_ylabel('Recall@10æ”¹å–„')
        axes[1,0].set_title('å¤„ç†æ—¶é—´ vs æ€§èƒ½æ”¹å–„')
        
        # 4. ä¸åŒKå€¼ä¸‹çš„æ”¹å–„åˆ†å¸ƒ
        k_improvements = []
        for k in [1, 5, 10, 20]:
            k_improvements.extend([(k, x) for x in results_df[f'recall_improvement@{k}']])
        
        k_df = pd.DataFrame(k_improvements, columns=['K', 'Improvement'])
        sns.boxplot(data=k_df, x='K', y='Improvement', ax=axes[1,1])
        axes[1,1].set_title('ä¸åŒKå€¼ä¸‹çš„Recallæ”¹å–„åˆ†å¸ƒ')
        axes[1,1].axhline(y=0, color='red', linestyle='--', alpha=0.5)
        
        plt.tight_layout()
        plt.savefig('parameter_optimization_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ° parameter_optimization_analysis.png")


def main():
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°SimHashé‡æ’åº")
    print("="*60)
    
    # 1. åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = BatchEvaluator()
    evaluator.load_data()
    
    # 2. æ¨¡æ‹Ÿæœç´¢ç»“æœ (åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›æ¥è‡ªSparklyçš„æœç´¢ç»“æœ)
    search_results = evaluator.simulate_search_results(k=50)
    print(f"âœ… ç”Ÿæˆäº† {len(search_results)} ä¸ªæŸ¥è¯¢çš„æœç´¢ç»“æœ")
    
    # 3. ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°
    results_df = evaluator.grid_search_parameters(search_results)
    
    # 4. ç”ŸæˆæŠ¥å‘Š
    evaluator.generate_report(results_df)
    
    print("\nğŸŠ æ‰¹é‡è¯„ä¼°å®Œæˆ!")
    print("ğŸ“„ æ£€æŸ¥ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ç»“æœ:")
    print("   - parameter_optimization_results.csv")
    print("   - parameter_optimization_analysis.png")


if __name__ == "__main__":
    main() 